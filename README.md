# AskQE: Answerability Check Extension

**This branch implements an answerability filter for the Vanilla question generation pipeline.** This extension addresses the trade-off between computational efficiency and question quality in the AskQE framework.

## Extension Overview

### Rationale

The original AskQE paper proposes two main question generation approaches:

1. **NLI (Atomic) Pipeline**: Uses a two-step process with fact extraction (API call) and entailment classification (off-the-shelf model). This is computationally expensive but produces high-quality, grounded questions.

2. **Vanilla Pipeline**: Generates questions directly from the source sentence without additional context. This is simpler and faster but more prone to hallucinations due to limited context.

### Our Proposal

We implement an **answerability filter** that:
- Takes questions generated by the Vanilla pipeline
- Classifies each question as "answerable" or "unanswerable"
- Removes hallucinated/unanswerable questions before the Question Answering step
- **Goal**: Improve Vanilla pipeline performance without adding significant computational overhead

### Experimental Results

**Key Finding**: Our experiments demonstrate that the answerability check **does NOT improve performance** relative to the Vanilla baseline, but importantly, **it does NOT degrade it either**.

The results are highly consistent, revealing an interesting insight:
> **"Unanswerable" questions are still discriminative for the AskQE pipeline.**

This suggests that even questions that appear to be hallucinated or poorly grounded can contribute meaningful signal for detecting translation errors. The noise introduced by these questions does not harm the overall quality estimation capability of the framework.

---

## Running the Experiments

This branch provides two interactive notebooks for experimentation. Both can be executed in Google Colab.

### Notebook 1: Pipeline Execution

**Purpose:** Execute the QG/QA pipeline with answerability filtering

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlessandroMaini/CucumBERT_askqe/blob/answerability-check/notebooks/answerability_check_pipeline.ipynb)

This notebook:
- Takes questions already generated by the Vanilla pipeline (from the main branch)
- Applies answerability classification using a classifier
- Filters out unanswerable questions
- Runs the QA step on filtered questions
- Evaluates performance using SBERT, string comparison metrics, and correlation analysis

**Data Source:** Questions pre-generated from the Vanilla pipeline in the main branch

### Notebook 2: Analysis and Evaluation

**Purpose:** Perform comprehensive analysis on the answerability check extension

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlessandroMaini/CucumBERT_askqe/blob/answerability-check/notebooks/answerability_check_analysis.ipynb)

This notebook:
- **Grounding Analysis**: Demonstrates that the Vanilla pipeline does generate hallucinated/unanswerable questions
- **Answerability Distribution**: Analyzes what percentage of questions are classified as unanswerable
- **Performance Comparison**: Compares Vanilla and Vanilla+Filter pipelines
- **Ablation Study**: Examines the discriminative power of "unanswerable" questions
- **Correlation Analysis**: Evaluates Pearson correlation with standard MT metrics
- **Visualization**: Plots results across different perturbation types and language pairs

---

## Key Insights

1. **Computational Efficiency**: The answerability filter adds minimal overhead compared to the full NLI pipeline
2. **Performance Stability**: Filtering does not degrade performance, maintaining parity with Vanilla
3. **Question Discriminativeness**: Even "bad" questions contribute to error detection
4. **Practical Implications**: For resource-constrained scenarios, the Vanilla pipeline without filtering may be sufficient

---